{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[homework question link](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/04-monitoring/homework.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-zoomcamp-playground/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.42244655)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "embedding_model.encode(answer_llm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Computing the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score(document):\n",
    "    original = embedding_model.encode(document['answer_orig'])\n",
    "    llm = embedding_model.encode(document['answer_llm'])\n",
    "    return original.dot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(17.515987)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_score(df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.apply(lambda x: evaluate_score(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384742\n",
       "min        4.547923\n",
       "25%       24.307844\n",
       "50%       28.336870\n",
       "75%       31.674309\n",
       "max       39.476013\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_norm_score(document):\n",
    "    original = embedding_model.encode(document['answer_orig'])\n",
    "    llm = embedding_model.encode(document['answer_llm'])\n",
    "\n",
    "    norm_original = np.sqrt((original * original).sum())\n",
    "    original = original / norm_original\n",
    "\n",
    "    norm_llm = np.sqrt((llm * llm).sum())\n",
    "    llm = llm / norm_llm\n",
    "\n",
    "    return original.dot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_score'] = df.apply(lambda x: evaluate_norm_score(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728393\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: norm_score, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['norm_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "r = df.iloc[10]\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Average rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.35490034990035496)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([x['f'] for x in scores.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_2(document):\n",
    "    answer_llm = document['answer_llm']\n",
    "    answer_orig = document['answer_orig']\n",
    "    return rouge_scorer.get_scores(answer_llm, answer_orig)[0]['rouge-2']['f']\n",
    "\n",
    "df['rouge-2'] = df.apply(lambda x: calc_rouge_2(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.206965\n",
       "std        0.153550\n",
       "min        0.000000\n",
       "25%        0.097809\n",
       "50%        0.178671\n",
       "75%        0.286181\n",
       "max        0.739130\n",
       "Name: rouge-2, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rouge-2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
